{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7dbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchfsdd import TorchFSDDGenerator, TrimSilence\n",
    "from torchaudio.transforms import MFCC, TimeMasking, FrequencyMasking\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# quick hacky way to get the package visible in the notebook\n",
    "base_path = Path(\".\").resolve().parent\n",
    "sys.path.append(str(base_path ))\n",
    "\n",
    "from sdc.rnn import RNN\n",
    "from sdc.trainer import RNNTrainer\n",
    "USE_GPU = torch.cuda.is_available() and 1\n",
    "N_CLASSES = 10 # 0-9 digits\n",
    "SAMPLING_RATE = 8e3  # 8kHz\n",
    "N_MFCC_CHANNELS = 13  # Number of MFCC channels\n",
    "MAX_EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\") if USE_GPU else torch.device(\"cpu\")\n",
    "CHECKPOINT_PATH = Path(\".\") / \"saved_models\"\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EXPERIMENT_NAME = \"base_rnn_model\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# torch.set_default_device(DEVICE)\n",
    "# torch.set_default_dtype(torch.float16)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "# chop off silence from the beginning and end of the audio\n",
    "trimmer = TrimSilence(threshold=1e-6)\n",
    "\n",
    "mfcc = MFCC(sample_rate=SAMPLING_RATE, n_mfcc=N_MFCC_CHANNELS)\n",
    "\n",
    "time_masking = TimeMasking(time_mask_param=2, p=0.3)\n",
    "freq_masking = FrequencyMasking(freq_mask_param=2)\n",
    "freq_masking.p = 0.3\n",
    "\n",
    "# Fetch the latest version of FSDD and initialize a generator with those files\n",
    "fsdd = TorchFSDDGenerator(version='master', transforms=None)\n",
    "\n",
    "# Create three Torch datasets for a train-validation-test split from the generator\n",
    "train_set, val_set, test_set = fsdd.train_val_test_split(test_size=0.15, val_size=0.15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da7ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ebe88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_root_dir = CHECKPOINT_PATH / EXPERIMENT_NAME\n",
    "default_root_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf0babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf_logger = MLFlowLogger(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    # tracking_uri=f\"http://localhost:5000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2577e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformation pipeline to apply to the recordings\n",
    "train_transforms = Compose([\n",
    "    trimmer,\n",
    "    mfcc,\n",
    "    time_masking,\n",
    "    freq_masking,\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    trimmer,\n",
    "    mfcc,\n",
    "])\n",
    "\n",
    "train_set.transforms = train_transforms\n",
    "val_set.transforms = val_transforms\n",
    "test_set.transforms = val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ab88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"Batch and pad wakeword data\"\"\"\n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    for d in data:\n",
    "        mfcc, label = d\n",
    "        print(mfcc.shape, label)\n",
    "        mfccs.append(mfcc.squeeze(0).transpose(0, 1))\n",
    "        labels.append(label)\n",
    "\n",
    "    # pad mfccs to ensure all tensors are same size in the time dim\n",
    "    mfccs = torch.nn.utils.rnn.pad_sequence(mfccs, batch_first=True)  # batch, seq_len, feature\n",
    "\n",
    "    # print(mfccs.shape)\n",
    "    labels = torch.asarray(labels)\n",
    "    return mfccs, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74cf0e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chanokin/innatera/audio_digit_classifier/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(\n",
    "    lstm_config={\n",
    "        'input_size': N_MFCC_CHANNELS,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 1,\n",
    "    },\n",
    "    linear_sizes=[N_CLASSES],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb354f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_trainer = RNNTrainer(\n",
    "    model=rnn,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445f69ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    default_root_dir=default_root_dir,\n",
    "    accelerator=\"cuda\" if USE_GPU else \"cpu\",\n",
    "    # amp_type=\"apex\",\n",
    "    # max_epochs=30 if i == 0 else 10,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    # strategy=\"ddp\",\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(),\n",
    "        # RichProgressBar(),\n",
    "        LearningRateMonitor(\"epoch\"),\n",
    "        # PlotWeightsOnEpochEndCallback(),\n",
    "        # PlotWeightsOnTrainStartCallback(),\n",
    "        # EarlyStopping(monitor=\"val_acc\", patience=15, mode=\"max\"),\n",
    "    ],\n",
    "    logger=mlf_logger,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b27e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model    | RNN                | 74.5 K | train\n",
      "1 | loss_fn  | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------\n",
      "74.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "74.5 K    Total params\n",
      "0.298     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chanokin/innatera/audio_digit_classifier/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 47]) 0\n",
      "torch.Size([13, 26]) 6\n",
      "torch.Size([13, 12]) 4\n",
      "torch.Size([13, 21]) 7\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]torch.Size([13, 21]) 1\n",
      "torch.Size([13, 18]) 5\n",
      "torch.Size([13, 10]) 3\n",
      "torch.Size([13, 12]) 4\n",
      "torch.Size([13, 18]) torch.Size([13, 18])7\n",
      " 7\n",
      "torch.Size([13, 11]) 1\n",
      "torch.Size([13, 27]) 1\n",
      "Training step 0\n",
      "Batch x shape: torch.Size([4, 47, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  8.13it/s]Training step 1\n",
      "Batch x shape: torch.Size([4, 21, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chanokin/innatera/audio_digit_classifier/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 22]) 7\n",
      "Training: |          | 0/? [00:00<?, ?it/s]torch.Size([13, 26]) 9\n",
      "torch.Size([13, 19]) 1\n",
      "Epoch 0:   0%|          | 0/540 [00:00<?, ?it/s] torch.Size([13, 22]) 5\n",
      "torch.Size([13, 13]) 7\n",
      "torch.Size([13, 8]) 6\n",
      "torch.Size([13, 16]) 8\n",
      "torch.Size([13, 13]) 3\n",
      "torch.Size([13, 11]) 1\n",
      "torch.Size([13, 23]) 3\n",
      "torch.Size([13, 18]) 7\n",
      "torch.Size([13, 11]) 5\n",
      "Training step 0\n",
      "Batch x shape: torch.Size([4, 16, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   0%|          | 1/540 [00:00<00:58,  9.24it/s, v_num=e486]13, 17]) 5\n",
      "torch.Size([13, 16]) 8\n",
      "torch.Size([13, 15]) 0Training step 1\n",
      "Batch x shape: torch.Size([4, 23, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   0%|          | 2/540 [00:00<00:30, 17.56it/s, v_num=e486]\n",
      "torch.Size([13, 11]) 3\n",
      "Training step 2\n",
      "Batch x shape: torch.Size([4, 17, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   1%|          | 3/540 [00:00<00:21, 24.74it/s, v_num=e486]torch.Size([13, 19]) 3\n",
      "torch.Size([13, 17]) 5\n",
      "torch.Size([13, 12]) 5\n",
      "torch.Size([13, 7]) 6\n",
      "Training step 3\n",
      "Batch x shape: torch.Size([4, 19, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   1%|          | 4/540 [00:00<00:51, 10.49it/s, v_num=e486]torch.Size([13, 17]) 1\n",
      "torch.Size([13, 21]) 6\n",
      "torch.Size([13, 15]) 6\n",
      "torch.Size([13, 12]) 3\n",
      "Training step 4\n",
      "Batch x shape: torch.Size([4, 21, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   1%|          | 5/540 [00:00<00:45, 11.81it/s, v_num=e486]torch.Size([13, 12]) 3\n",
      "torch.Size([13, 17]) 0\n",
      "torch.Size([13, 8]) 6\n",
      "torch.Size([13, 21]) 0\n",
      "Training step 5\n",
      "Batch x shape: torch.Size([4, 21, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   1%|          | 6/540 [00:00<00:41, 12.87it/s, v_num=e486]torch.Size([13, 19]) 3\n",
      "torch.Size([13, 12]) 8\n",
      "torch.Size([13, 18]) 9\n",
      "torch.Size([13, 12]) 2\n",
      "Training step 6\n",
      "Batch x shape: torch.Size([4, 19, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   1%|▏         | 7/540 [00:00<00:38, 13.93it/s, v_num=e486]torch.Size([13, 16]) 3\n",
      "torch.Size([13, 32]) 0\n",
      "torch.Size([13, 15]) 7\n",
      "torch.Size([13, 17]) 9\n",
      "Training step 7\n",
      "Batch x shape: torch.Size([4, 32, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   1%|▏         | 8/540 [00:00<00:36, 14.67it/s, v_num=e486]torch.Size([13, 18]) 4\n",
      "torch.Size([13, 16]) 2\n",
      "torch.Size([13, 19]) 7\n",
      "torch.Size([13, 24]) 4\n",
      "Training step 8\n",
      "Batch x shape: torch.Size([4, 24, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   2%|▏         | 9/540 [00:00<00:34, 15.26it/s, v_num=e486]torch.Size([13, 17]) 1\n",
      "torch.Size([13, 18]) 2\n",
      "torch.Size([13, 19]) 0\n",
      "torch.Size([13, 22]) 6\n",
      "Training step 9\n",
      "Batch x shape: torch.Size([4, 22, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   2%|▏         | 10/540 [00:00<00:33, 15.81it/s, v_num=e486]torch.Size([13, 26]) 1\n",
      "torch.Size([13, 18]) 4\n",
      "torch.Size([13, 24]) 0\n",
      "torch.Size([13, 16]) 5\n",
      "Training step 10\n",
      "Batch x shape: torch.Size([4, 26, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   2%|▏         | 11/540 [00:00<00:32, 16.21it/s, v_num=e486]torch.Size([13, 19]) 4\n",
      "torch.Size([13, 14]) 8\n",
      "torch.Size([13, 24]) 7\n",
      "torch.Size([13, 12]) 4\n",
      "Training step 11\n",
      "Batch x shape: torch.Size([4, 24, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   2%|▏         | 12/540 [00:00<00:31, 16.66it/s, v_num=e486]torch.Size([13, 16]) 3\n",
      "torch.Size([13, 13]) 2\n",
      "torch.Size([13, 27]) 0\n",
      "torch.Size([13, 15]) 9\n",
      "Training step 12\n",
      "Batch x shape: torch.Size([4, 27, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   2%|▏         | 13/540 [00:00<00:30, 17.14it/s, v_num=e486]torch.Size([13, 12]) 1\n",
      "torch.Size([13, 20]) 1\n",
      "torch.Size([13, 14]) 3\n",
      "torch.Size([13, 24]) 8\n",
      "Training step 13\n",
      "Batch x shape: torch.Size([4, 24, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   3%|▎         | 14/540 [00:00<00:29, 17.59it/s, v_num=e486]torch.Size([13, 40]) 6\n",
      "torch.Size([13, 15]) 9\n",
      "torch.Size([13, 21]) 2\n",
      "torch.Size([13, 40]) 7\n",
      "Training step 14\n",
      "Batch x shape: torch.Size([4, 40, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   3%|▎         | 15/540 [00:00<00:29, 17.67it/s, v_num=e486]torch.Size([13, 13]) 2\n",
      "torch.Size([13, 12]) 1\n",
      "torch.Size([13, 17]) 8\n",
      "torch.Size([13, 17]) 0\n",
      "Training step 15\n",
      "Batch x shape: torch.Size([4, 17, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   3%|▎         | 16/540 [00:00<00:28, 18.20it/s, v_num=e486]torch.Size([13, 17]) 7\n",
      "torch.Size([13, 14]) 4\n",
      "torch.Size([13, 24]) 5\n",
      "torch.Size([13, 17]) 8\n",
      "Training step 16\n",
      "Batch x shape: torch.Size([4, 24, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   3%|▎         | 17/540 [00:00<00:28, 18.58it/s, v_num=e486]torch.Size([13, 18]) 9\n",
      "torch.Size([13, 24]) 4\n",
      "torch.Size([13, 25]) 1\n",
      "torch.Size([13, 14]) 4\n",
      "Training step 17\n",
      "Batch x shape: torch.Size([4, 25, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   3%|▎         | 18/540 [00:00<00:27, 18.78it/s, v_num=e486]torch.Size([13, 16]) 1\n",
      "torch.Size([13, 17]) 3\n",
      "torch.Size([13, 17]) 1\n",
      "torch.Size([13, 22]) 2\n",
      "Training step 18\n",
      "Batch x shape: torch.Size([4, 22, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   4%|▎         | 19/540 [00:01<00:27, 18.93it/s, v_num=e486]torch.Size([13, 20]) 5\n",
      "torch.Size([13, 13]) 8\n",
      "torch.Size([13, 13]) 4\n",
      "torch.Size([13, 20]) 4\n",
      "Training step 19\n",
      "Batch x shape: torch.Size([4, 20, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   4%|▎         | 20/540 [00:01<00:27, 19.04it/s, v_num=e486]torch.Size([13, 18]) 5\n",
      "torch.Size([13, 18]) 2\n",
      "torch.Size([13, 11]) 6\n",
      "torch.Size([13, 28]) 6\n",
      "Training step 20\n",
      "Batch x shape: torch.Size([4, 28, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   4%|▍         | 21/540 [00:01<00:27, 19.05it/s, v_num=e486]torch.Size([13, 17]) 9\n",
      "torch.Size([13, 24]) 1\n",
      "torch.Size([13, 18]) 4\n",
      "torch.Size([13, 19]) 7\n",
      "Training step 21\n",
      "Batch x shape: torch.Size([4, 24, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   4%|▍         | 22/540 [00:01<00:27, 19.16it/s, v_num=e486]torch.Size([13, 30]) 1\n",
      "torch.Size([13, 21]) 2\n",
      "torch.Size([13, 14]) 2\n",
      "torch.Size([13, 16]) 8\n",
      "Training step 22\n",
      "Batch x shape: torch.Size([4, 30, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   4%|▍         | 23/540 [00:01<00:26, 19.24it/s, v_num=e486]torch.Size([13, 20]) 2\n",
      "torch.Size([13, 19]) 9\n",
      "torch.Size([13, 17]) 9\n",
      "torch.Size([13, 19]) 0\n",
      "Training step 23\n",
      "Batch x shape: torch.Size([4, 20, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   4%|▍         | 24/540 [00:01<00:26, 19.40it/s, v_num=e486]torch.Size([13, 12]) 6\n",
      "torch.Size([13, 13]) 3\n",
      "torch.Size([13, 21]) 4\n",
      "torch.Size([13, 22]) 8\n",
      "Training step 24\n",
      "Batch x shape: torch.Size([4, 22, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   5%|▍         | 25/540 [00:01<00:26, 19.58it/s, v_num=e486]torch.Size([13, 19]) 0\n",
      "torch.Size([13, 35]) 7\n",
      "torch.Size([13, 19]) 6\n",
      "torch.Size([13, 16]) 4\n",
      "Training step 25\n",
      "Batch x shape: torch.Size([4, 35, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   5%|▍         | 26/540 [00:01<00:26, 19.65it/s, v_num=e486]torch.Size([13, 23]) 2\n",
      "torch.Size([13, 18]) 8\n",
      "torch.Size([13, 11]) 8\n",
      "torch.Size([13, 20]) 7\n",
      "Training step 26\n",
      "Batch x shape: torch.Size([4, 23, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   5%|▌         | 27/540 [00:01<00:25, 19.81it/s, v_num=e486]torch.Size([13, 11]) 1\n",
      "torch.Size([13, 15]) 1\n",
      "torch.Size([13, 14]) 0\n",
      "torch.Size([13, 33]) 9\n",
      "Training step 27\n",
      "Batch x shape: torch.Size([4, 33, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   5%|▌         | 28/540 [00:01<00:25, 19.95it/s, v_num=e486]torch.Size([13, 25]) 6\n",
      "torch.Size([13, 14]) 8\n",
      "torch.Size([13, 9]) 6\n",
      "torch.Size([13, 14]) 2\n",
      "Training step 28\n",
      "Batch x shape: torch.Size([4, 25, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   5%|▌         | 29/540 [00:01<00:25, 20.15it/s, v_num=e486]torch.Size([13, 24]) 8\n",
      "torch.Size([13, 17]) 7\n",
      "torch.Size([13, 18]) 5\n",
      "torch.Size([13, 17]) 5\n",
      "Training step 29\n",
      "Batch x shape: torch.Size([4, 24, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   6%|▌         | 30/540 [00:01<00:25, 20.27it/s, v_num=e486]torch.Size([13, 10]) 6\n",
      "torch.Size([13, 22]) 8\n",
      "torch.Size([13, 26]) 0\n",
      "torch.Size([13, 26]) 9\n",
      "Training step 30\n",
      "Batch x shape: torch.Size([4, 26, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   6%|▌         | 31/540 [00:01<00:25, 20.35it/s, v_num=e486]torch.Size([13, 25]) 9\n",
      "torch.Size([13, 13]) 4\n",
      "torch.Size([13, 21]) 5\n",
      "torch.Size([13, 17]) 5\n",
      "Training step 31\n",
      "Batch x shape: torch.Size([4, 25, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   6%|▌         | 32/540 [00:01<00:24, 20.51it/s, v_num=e486]torch.Size([13, 19]) 7\n",
      "torch.Size([13, 23]) 0\n",
      "torch.Size([13, 17]) 5\n",
      "torch.Size([13, 15]) 9\n",
      "Training step 32\n",
      "Batch x shape: torch.Size([4, 23, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   6%|▌         | 33/540 [00:01<00:24, 20.68it/s, v_num=e486]torch.Size([13, 14]) 1\n",
      "torch.Size([13, 18]) 8\n",
      "torch.Size([13, 20]) 0\n",
      "torch.Size([13, 21]) 0\n",
      "Training step 33\n",
      "Batch x shape: torch.Size([4, 21, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   6%|▋         | 34/540 [00:01<00:24, 20.81it/s, v_num=e486]torch.Size([13, 17]) 5\n",
      "torch.Size([13, 14]) 8\n",
      "torch.Size([13, 10]) 8\n",
      "torch.Size([13, 35]) 9\n",
      "Training step 34\n",
      "Batch x shape: torch.Size([4, 35, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   6%|▋         | 35/540 [00:01<00:24, 20.96it/s, v_num=e486]torch.Size([13, 18]) 8\n",
      "torch.Size([13, 16]) 7\n",
      "torch.Size([13, 23]) 1\n",
      "torch.Size([13, 17]) 0\n",
      "Training step 35\n",
      "Batch x shape: torch.Size([4, 23, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   7%|▋         | 36/540 [00:01<00:23, 21.11it/s, v_num=e486]torch.Size([13, 27]) 3\n",
      "torch.Size([13, 23]) 4\n",
      "torch.Size([13, 14]) 2\n",
      "torch.Size([13, 21]) 4\n",
      "Training step 36\n",
      "Batch x shape: torch.Size([4, 27, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   7%|▋         | 37/540 [00:01<00:23, 21.21it/s, v_num=e486]torch.Size([13, 18]) 0\n",
      "torch.Size([13, 14]) 3\n",
      "torch.Size([13, 12]) 6\n",
      "torch.Size([13, 14]) 2\n",
      "Training step 37\n",
      "Batch x shape: torch.Size([4, 18, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   7%|▋         | 38/540 [00:01<00:23, 21.42it/s, v_num=e486]torch.Size([13, 14]) 3\n",
      "torch.Size([13, 15]) 2\n",
      "torch.Size([13, 22]) 6\n",
      "torch.Size([13, 18]) 9\n",
      "Training step 38\n",
      "Batch x shape: torch.Size([4, 22, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   7%|▋         | 39/540 [00:01<00:23, 21.56it/s, v_num=e486]torch.Size([13, 18]) 6\n",
      "torch.Size([13, 20]) 5\n",
      "torch.Size([13, 17]) 2\n",
      "torch.Size([13, 20]) 4\n",
      "Training step 39\n",
      "Batch x shape: torch.Size([4, 20, 13])\n",
      "Batch y shape: torch.Size([4])\n",
      "Epoch 0:   7%|▋         | 40/540 [00:01<00:23, 21.69it/s, v_num=e486]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=rnn_trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d748740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab49bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728026a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089bc67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b967c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516d9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4d69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da5dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
